## [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)

### Abstract
The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task,improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.

### 1 Introduction
Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].

Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.

Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.


## Translation1


### Аннотация

Доминирующие модели преобразования последовательностей основаны на сложных рекуррентных или сверточных нейронных сетях, которые включают кодировщик и декодировщик. Наилучшие модели также соединяют кодировщик и декодировщик с помощью механизма внимания. Мы предлагаем новую простую архитектуру сети, Трансформер, основанную исключительно на механизмах внимания, полностью отказываясь от рекуррентности и сверток. Эксперименты на двух задачах машинного перевода показывают, что эти модели превосходят по качеству, при этом будучи более параллелизуемыми и требующими значительно меньше времени для обучения. Наша модель достигает 28,4 BLEU на задаче перевода с английского на немецкий WMT 2014, улучшая существующие лучшие результаты, включая ансамбли, более чем на 2 BLEU. На задаче перевода с английского на французский WMT 2014 наша модель устанавливает новый state-of-the-art BLEU балл для одной модели в 41,8 после обучения в течение 3,5 дней на восьми GPU, что составляет малую долю затрат на обучение лучших моделей из литературы. Мы показываем, что Трансформер хорошо обобщается на другие задачи, успешно применяя его к синтаксическому анализу составляющих английского языка как с большими, так и с ограниченными обучающими данными.

### 1 Введение

Рекуррентные нейронные сети, в частности, долгосрочная краткосрочная память [13] и gated recurrent [7] нейронные сети, прочно утвердились как state-of-the-art подходы в моделировании последовательностей и задачах преобразования, таких как языковое моделирование и машинный перевод [35, 2, 5]. Многочисленные усилия с тех пор продолжают раздвигать границы рекуррентных языковых моделей и архитектур кодировщик-декодировщик [38, 24, 15].

Рекуррентные модели обычно разлагают вычисления по позициям символов во входных и выходных последовательностях. Соотнося позиции с шагами во времени вычислений, они генерируют последовательность скрытых состояний ht как функцию предыдущего скрытого состояния ht−1 и входа для позиции t. Эта по своей сути последовательная природа исключает параллелизацию внутри обучающих примеров, что становится критическим при более длинных последовательностях, поскольку ограничения памяти ограничивают пакетную обработку примеров. Недавние работы достигли значительных улучшений в вычислительной эффективности за счет трюков с разложением [21] и условных вычислений [32], при этом также улучшая производительность модели в случае последнего. Однако фундаментальное ограничение последовательных вычислений остается.

Механизмы внимания стали неотъемлемой частью убедительных моделей моделирования последовательностей и преобразования в различных задачах, позволяя моделировать зависимости без учета их расстояния во входных или выходных последовательностях [2, 19]. Однако во всех случаях, за исключением нескольких [27], такие механизмы внимания используются в сочетании с рекуррентной сетью.


## Translation2

### Аннотация
Доминирующие модели передачи последовательности основаны на сложных рекуррентных или конволюционных нейронных сетях, включающих кодер и декодер. Наиболее эффективные модели также соединяют кодер и декодер через механизм внимания. Мы предлагаем новую простую сетевую архитектуру «Трансформатор», основанную исключительно на механизмах внимания и полностью отказывающуюся от рекуррентных и конволюционных механизмов. Эксперименты на двух задачах машинного перевода показали, что эти модели превосходят по качеству, при этом они лучше распараллеливаются и требуют значительно меньше времени на обучение. В задаче англо-немецкого перевода WMT 2014 наша модель достигает 28,4 BLEU, что более чем на 2 BLEU лучше существующих лучших результатов, включая ансамбли. На задаче перевода с английского на французский в рамках WMT 2014 наша модель показывает новый рекордный результат BLEU для одной модели - 41,8 балла после обучения в течение 3,5 дней на восьми графических процессорах, что составляет лишь малую часть затрат на обучение лучших моделей из литературы. Мы показываем, что трансформатор хорошо обобщается на другие задачи, успешно применяя его для синтаксического разбора английского языка как на больших, так и на ограниченных обучающих данных.

### 1 Введение
Рекуррентные нейронные сети, в частности нейронные сети с долговременной кратковременной памятью [13] и рекуррентные нейронные сети с воротами [7], прочно утвердились в качестве передовых подходов к моделированию последовательностей и решению задач трансдукции, таких как моделирование языка и машинный перевод [35, 2, 5]. С тех пор многочисленные усилия продолжают расширять границы рекуррентных языковых моделей и архитектур кодеров-декодеров [38, 24, 15].

Рекуррентные модели обычно распределяют вычисления по позициям символов во входных и выходных последовательностях. Приравнивая позиции к шагам во времени вычислений, они генерируют последовательность скрытых состояний ht как функцию предыдущего скрытого состояния ht-1 и входа для позиции t. Эта последовательная природа исключает распараллеливание в обучающих примерах, что становится критичным при больших длинах последовательностей, поскольку ограничения памяти ограничивают пакетную обработку примеров. В последних работах удалось добиться значительного повышения эффективности вычислений за счет трюков факторизации [21] и условных вычислений [32], а также улучшить производительность модели в случае последних. Однако фундаментальное ограничение, связанное с последовательными вычислениями, остается.

Механизмы внимания стали неотъемлемой частью моделирования последовательности и моделей трансдукции в различных задачах, позволяя моделировать зависимости без учета их расстояния во входной или выходной последовательности [2, 19]. Однако во всех случаях, за исключением нескольких [27], такие механизмы внимания используются в сочетании с рекуррентной сетью.