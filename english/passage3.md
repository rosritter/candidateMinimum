## [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/pdf/2006.11477)

### Abstract
We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.1

### 1. Introduction 
Neural networks benefit from large quantities of labeled training data. However, in many settings labeled data is much harder to come by than unlabeled data: current speech recognition systems require thousands of hours of transcribed speech to reach acceptable performance which is not available for the vast majority of the nearly 7,000 languages spoken worldwide [31]. Learning purely from labeled examples does not resemble language acquisition in humans: infants learn language by listening to adults around them - a process that requires learning good representations of speech. 

In machine learning, self-supervised learning has emerged as a paradigm to learn general data representations from unlabeled examples and to fine-tune the model on labeled data. This has been particularly successful for natural language processing [43, 45, 9] and is an active research area for computer vision [20, 2, 36, 19, 6]. 

In this paper, we present a framework for self-supervised learning of representations from raw audio data. Our approach encodes speech audio via a multi-layer convolutional neural network and then masks spans of the resulting latent speech representations [26, 56], similar to masked language modeling [9]. The latent representations are fed to a Transformer network to build contextualized representations and the model is trained via a contrastive task where the true latent is to be distinguished from distractors [54, 49, 48, 28] (§ 2). 

As part of training, we learn discrete speech units [53, 32, 7, 18] via a gumbel softmax [24, 5] to represent the latent representations in the contrastive task (Figure 1) which we find to be more effective than non-quantized targets. After pre-training on unlabeled speech, the model is fine-tuned on labeled data with a Connectionist Temporal Classification (CTC) loss [14, 4] to be used for downstream speech recognition tasks (§ 3)

## Translation

### Аннотация
Мы впервые показываем, что обучение мощных представлений только на аудиозаписях речи с последующей тонкой настройкой на транскрибированной речи может превзойти лучшие полусупервизорные методы, будучи при этом концептуально более простым. wav2vec 2.0 маскирует речевой вход в латентном пространстве и решает контрастную задачу, определенную над квантованием латентных представлений, которые совместно обучаются. Эксперименты с использованием всех помеченных данных Librispeech достигают 1,8/3,3 WER на чистых/других тестовых наборах. При уменьшении количества помеченных данных до одного часа wav2vec 2.0 превосходит предыдущий уровень техники на 100-часовом подмножестве, используя при этом в 100 раз меньше помеченных данных. При использовании всего десяти минут меченых данных и предварительном обучении на 53 тыс. часов немеченых данных WER достигает 4,8/8,2. Это демонстрирует возможность распознавания речи при ограниченном количестве помеченных данных.1 

### 1 Введение

Нейронные сети выигрывают от большого количества помеченных обучающих данных. Однако во многих случаях получить маркированные данные гораздо сложнее, чем немаркированные: современные системы распознавания речи требуют тысячи часов транскрибированной речи для достижения приемлемой производительности, что недоступно для подавляющего большинства из почти 7 000 языков, на которых говорят во всем мире [31]. Обучение исключительно на маркированных примерах не похоже на освоение языка человеком: младенцы учат язык, слушая окружающих их взрослых - процесс, требующий изучения хороших представлений речи. 

В машинном обучении самоподдерживающееся обучение стало парадигмой для изучения общих представлений данных на немаркированных примерах и точной настройки модели на маркированных данных. Это было особенно успешно реализовано в обработке естественного языка [43, 45, 9] и является активной областью исследований в компьютерном зрении [20, 2, 36, 19, 6]. 

В этой статье мы представляем структуру для самоконтролируемого обучения представлений на основе необработанных аудиоданных. Наш подход кодирует речевые аудиоданные с помощью многослойной конволюционной нейронной сети, а затем маскирует участки полученных латентных представлений речи [26, 56], аналогично моделированию языка по маске [9]. Латентные представления поступают в сеть-трансформер для построения контекстуализированных представлений, и модель обучается с помощью контрастной задачи, в которой истинное латентное представление должно быть отличимо от отвлекающих [54, 49, 48, 28] (§ 2). 

В процессе обучения мы изучаем дискретные речевые единицы [53, 32, 7, 18] с помощью софтмакса Гумбеля [24, 5] для представления латентных репрезентаций в контрастной задаче (рис. 1), что, по нашему мнению, более эффективно, чем неквантованные цели. После предварительного обучения на немаркированной речи модель настраивается на маркированных данных с помощью коннекционистской темпоральной классификации (CTC) [14, 4] для использования в последующих задачах распознавания речи (§ 3).