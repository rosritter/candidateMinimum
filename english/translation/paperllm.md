## Robust Speech Recognition via Large-Scale Weak Supervision

Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever


### Перевод страниц 1-4

## Надежное распознавание речи с помощью слабого контроля большого масштаба


### Аннотация

Мы изучаем возможности систем обработки речи, обученных простым предсказанием больших объемов транскриптов аудио из интернета. При масштабировании до 680 000 часов многоязычного и многозадачного обучения такие модели хорошо обобщаются на стандартных тестах и часто конкурируют с предыдущими результатами, полученными при полном обучении с учителем, но в условиях нулевого переноса без необходимости тонкой настройки. По сравнению с людьми, эти модели приближаются к их точности и устойчивости. Мы публикуем модели и код для вывода, чтобы они послужили основой для дальнейших исследований в области надежной обработки речи.

Прогресс в распознавании речи был стимулирован развитием методов предварительного обучения без учителя, таких как Wav2Vec 2.0 (Baevski et al., 2020). Поскольку эти методы обучаются непосредственно на необработанном аудио без необходимости человеческих меток, они могут эффективно использовать большие наборы данных немаркированной речи и были быстро масштабированы до 1 000 000 часов обучающих данных (Zhang et al., 2021), что значительно превышает примерно 1000 часов, типичных для академических наборов данных с учителем. При тонкой настройке на стандартных тестах этот подход улучшил состояние дел в отрасли, особенно в условиях ограниченных данных.

Эти предварительно обученные аудиокодеры формируют высококачественные представления речи, но, поскольку они полностью не используют учителя, им не хватает эквивалентно производительного декодера, который бы преобразовывал эти представления в полезные выходные данные, что требует этапа тонкой настройки для выполнения таких задач, как распознавание речи. Это, к сожалению, ограничивает их полезность и влияние, поскольку тонкая настройка остается сложным процессом, требующим квалифицированного специалиста. Существует также дополнительный риск, связанный с необходимостью тонкой настройки. Методы машинного обучения чрезвычайно искусны в поиске закономерностей в обучающих наборах данных, которые повышают производительность на отложенных данных из того же набора. Однако некоторые из этих закономерностей хрупкие и ложные, и не обобщаются на другие наборы данных и распределения. В особенно тревожном примере Radford et al. (2021) зафиксировали увеличение точности классификации объектов на 9,2% при тонкой настройке модели компьютерного зрения на наборе данных ImageNet (Russakovsky et al., 2015) без какого-либо улучшения средней точности при классификации тех же объектов на семи других наборах данных с естественными изображениями. Модель, которая достигает «сверхчеловеческой» производительности при обучении на одном наборе данных, все еще может совершать множество базовых ошибок при оценке на другом, возможно, именно потому, что она использует специфические особенности этого набора данных, которые незаметны для человека (Geirhos et al., 2020).

Это говорит о том, что хотя предварительное обучение без учителя значительно улучшило качество аудиокодеров, отсутствие эквивалентно качественного предварительно обученного декодера в сочетании с рекомендуемым протоколом тонкой настройки, специфичной для набора данных, является критической слабостью, которая ограничивает их полезность и устойчивость. Целью системы распознавания речи должно быть надежное функционирование «прямо из коробки» в широком диапазоне условий без необходимости контролируемой тонкой настройки декодера для каждого распределения при развертывании.

Как показали Narayanan et al. (2018), Likhomanenko et al. (2020) и Chan et al. (2021), системы распознавания речи, предварительно обученные с учителем на множестве наборов данных/доменов, демонстрируют более высокую устойчивость и гораздо лучше обобщаются на отложенные наборы данных, чем модели, обученные на одном источнике. Эти работы достигают этого, объединяя как можно больше существующих высококачественных наборов данных для распознавания речи. Однако объем таких данных, легко доступных, все еще ограничен. SpeechStew (Chan et al., 2021) объединяет 7 уже существующих наборов данных, в общей сложности 5140 часов обучения с учителем. Хотя это не мало, это все еще ничтожно мало по сравнению с упомянутыми ранее 1 000 000 часов немаркированных данных речи, использованных в Zhang et al. (2021).

Признавая ограниченный размер существующих высококачественных наборов данных с учителем, недавние усилия были направлены на создание более крупных наборов данных для распознавания речи. Ослабляя требование к золотому стандарту транскриптов, проверенных человеком, Chen et al. (2021) и Galvez et al. (2021) используют сложные автоматизированные процессы для масштабирования слабоконтролируемого распознавания речи до 10 000 и 30 000 часов более шумных обучающих данных. Такой компромисс между качеством и количеством часто оправдан. Хотя это пока недостаточно изучено для распознавания речи, недавние работы в области компьютерного зрения показали, что переход от стандартных краудсорсинговых наборов данных, таких как ImageNet (Russakovsky et al., 2015), к значительно большим, но слабоконтролируемым наборам данных существенно улучшает устойчивость и обобщение моделей (Mahajan et al., 2018; Kolesnikov et al., 2020).

Тем не менее, эти новые наборы данных лишь в несколько раз больше суммы существующих высококачественных наборов данных и все еще значительно меньше, чем предыдущие работы без учителя. В этой работе мы устраняем этот разрыв, масштабируя слабоконтролируемое распознавание речи на следующий порядок величины до 680 000 часов маркированных аудиоданных. Мы называем наш подход Whisper2. Мы демонстрируем, что модели, обученные в таком масштабе, хорошо переносятся на существующие наборы данных в режиме нулевого переноса, устраняя необходимость в специфической для набора данных тонкой настройке для достижения высококачественных результатов.

Помимо масштаба, наша работа также сосредоточена на расширении охвата слабоконтролируемого предварительного обучения за пределы распознавания речи только на английском языке, чтобы оно стало многоязычным и многозадачным. Из этих 680 000 часов аудио 117 000 часов охватывают 96 других языков. Набор данных также включает 125 000 часов данных перевода с языка X на английский. Мы обнаружили, что для достаточно больших моделей нет недостатков, а даже есть преимущества в совместном многоязычном и многозадачном обучении.

Наша работа предполагает, что простое масштабирование слабоконтролируемого предварительного обучения до сих пор недооценивалось для распознавания речи. Мы достигаем этих результатов без необходимости использования методов самоконтроля или самообучения, которые были основой недавних крупномасштабных работ по распознаванию речи. Чтобы послужить основой для дальнейших исследований надежного распознавания речи, мы публикуем код вывода и модели по следующему URL: https://github.com/openai/whisper.

---

### 2. Подход

#### 2.1. Обработка данных

Следуя тенденции недавних работ, использующих текст веб-масштаба из интернета для обучения систем машинного обучения, мы применяем минималистичный подход к предварительной обработке данных. В отличие от многих работ по распознаванию речи, мы обучаем модели Whisper предсказывать необработанный текст транскриптов без значительной стандартизации, полагаясь на выразительность моделей последовательности в последовательность, чтобы они научились сопоставлять высказывания и их транскрибированную форму. Это упрощает процесс распознавания речи, поскольку устраняет необходимость в отдельном шаге обратной нормализации текста для создания естественных транскрипций.

Мы создаем набор данных из аудио, которое связано с транскриптами в интернете. Это приводит к очень разнообразному набору данных, охватывающему широкий спектр аудио из разных сред, настроек записи, говорящих и языков. Хотя разнообразие качества аудио может помочь обучить модель быть устойчивой, разнообразие качества транскриптов не приносит аналогичной пользы. Первоначальная проверка показала большое количество некачественных транскриптов в исходном наборе данных. Чтобы решить эту проблему, мы разработали несколько автоматизированных методов фильтрации для улучшения качества транскриптов.

Многие транскрипты в интернете на самом деле не созданы человеком, а являются результатом работы существующих систем автоматического распознавания речи (ASR). Недавние исследования показали, что обучение на наборах данных, смешанных из данных, созданных человеком и машиной, может значительно ухудшить производительность систем перевода (Ghorbani et al., 2021). Чтобы избежать изучения «транскриптного языка», мы разработали множество эвристик для обнаружения и удаления машинно-сгенерированных транскриптов из обучающего набора данных. Многие существующие системы ASR выводят только ограниченное подмножество письменного языка, удаляя или нормализуя аспекты, которые трудно предсказать только по аудиосигналам, такие как сложная пунктуация (восклицательные знаки, запятые и вопросительные знаки), форматирование пробелов, например абзацы, или стилистические аспекты, такие как заглавные буквы. Полностью прописной или полностью строчный транскрипт вряд ли создан человеком. Хотя многие системы ASR включают некоторый уровень обратной нормализации текста, он часто прост или основан на правилах и все еще различим по другим необработанным аспектам, таким как отсутствие запятых.

Мы также используем детектор языка аудио, который был создан путем тонкой настройки прототипной модели, обученной на прототипной версии набора данных, на VoxLingua107 (Valk & Alumae¨, 2021), чтобы убедиться, что разговорный язык соответствует языку транскрипта согласно CLD2. Если они не совпадают, мы не включаем пару (аудио, транскрипт) как пример обучения распознаванию речи в набор данных. Мы делаем исключение, если язык транскрипта — английский, и добавляем эти пары в набор данных как примеры обучения перевода речи с языка X на английский. Мы используем нечеткое удаление дубликатов текстов транскриптов, чтобы уменьшить количество дублирования и автоматически сгенерированного контента в обучающем наборе данных.

Мы разбиваем аудиофайлы на 30-секундные сегменты, связанные с подмножеством транскрипта, который происходит в этом временном сегменте. Мы обучаем на всем аудио, включая сегменты, где нет речи (хотя с уменьшенной вероятностью выборки), и используем эти сегменты как обучающие данные для обнаружения голосовой активности.

Для дополнительного прохода фильтрации после обучения начальной модели мы собрали информацию о ее уровне ошибок на источниках обучающих данных и провели ручную проверку этих источников, сортируя их по комбинации высокого уровня ошибок и размера источника данных, чтобы эффективно выявлять и удалять низкокачественные источники. Эта проверка показала большое количество только частично транскрибированных или плохо выровненных/неправильно выровненных транскриптов, а также оставшиеся низкокачественные машинно-сгенерированные субтитры, которые эвристики фильтрации не обнаружили.

Чтобы избежать загрязнения, мы проводим удаление дубликатов на уровне транскриптов между обучающим набором данных и наборами данных для оценки, которые, по нашему мнению, имеют более высокий риск пересечения, а именно TED-LIUM 3 (Hernandez et al., 2018).


### 2. Подход

#### 2.2. Модель

Поскольку наша работа сосредоточена на изучении возможностей крупномасштабного обучения с учителем для распознавания речи, мы используем стандартную архитектуру, чтобы избежать смешивания наших выводов с улучшениями модели. Мы выбрали трансформер с энкодером и декодером (Vaswani et al., 2017), так как эта архитектура хорошо зарекомендовала себя в плане надежного масштабирования. Все аудио передискретизируется до 16 000 Гц, и вычисляется 80-канальная логарифмически масштабированная мел-спектрограмма на окнах длительностью 25 миллисекунд с шагом 10 миллисекунд. Для нормализации признаков мы глобально масштабируем входные данные так, чтобы они находились в диапазоне от -1 до 1 с приблизительно нулевым средним значением по всему набору данных предварительного обучения. Энкодер обрабатывает это представление входных данных с помощью небольшого начального блока, состоящего из двух сверточных слоев с шириной фильтра 3 и функцией активации GELU (Hendrycks & Gimpel, 2016), где второй сверточный слой имеет шаг два. Затем к выходу начального блока добавляются синусоидальные позиционные вложения, после чего применяются блоки трансформера энкодера. Трансформер использует блоки с предварительной активацией и остаточными связями (Child et al., 2019), а к выходу энкодера применяется финальная нормализация слоя. Декодер использует выученные позиционные вложения и связанные представления входных и выходных токенов (Press & Wolf, 2017). Энкодер и декодер имеют одинаковую ширину и количество блоков трансформера. Архитектура модели представлена на рисунке 1.

Для моделей, работающих только с английским языком, мы используем тот же байтовый BPE-токенизатор текста, что и в GPT-2 (Sennrich et al., 2015; Radford et al., 2019), а для многоязычных моделей мы перестраиваем словарь (но сохраняем его размер), чтобы избежать чрезмерной фрагментации на других языках, поскольку словарь BPE в GPT-2 ориентирован только на английский язык.

---

#### 2.3. Многозадачный формат

Хотя предсказание слов, произнесенных в данном аудиофрагменте, является ключевой частью полной задачи распознавания речи и широко изучается в исследованиях, это не единственная ее составляющая. Полнофункциональная система распознавания речи может включать множество дополнительных компонентов, таких как обнаружение голосовой активности, разделение говорящих и обратная нормализация текста. Эти компоненты часто обрабатываются отдельно, что приводит к относительно сложной системе вокруг основной модели распознавания речи. Чтобы снизить эту сложность, мы хотели бы, чтобы одна модель выполняла весь процесс обработки речи, а не только основную часть распознавания. Важным аспектом здесь является интерфейс модели. Существует множество различных задач, которые могут быть выполнены на одном и том же входном аудиосигнале: транскрипция, перевод, обнаружение голосовой активности, выравнивание и идентификация языка — вот некоторые примеры.

Для работы такого рода отображения "один ко многим" с использованием одной модели необходимо указание задачи. Мы используем простой формат для указания всех задач и условной информации в виде последовательности входных токенов для декодера. Поскольку наш декодер представляет собой языковую модель, зависящую от аудио, мы также обучаем его учитывать историю текста транскрипта в надежде, что он научится использовать более длинный текстовый контекст для разрешения неоднозначностей в аудио. В частности, с некоторой вероятностью мы добавляем текст транскрипта, предшествующий текущему аудиосегменту, в контекст декодера. Мы обозначаем начало предсказания токеном <|startoftranscript|>. Сначала мы предсказываем язык речи, который представлен уникальным токеном для каждого языка в нашем обучающем наборе данных (всего 99). Эти языковые метки получены из упомянутой ранее модели VoxLingua107. В случае отсутствия речи в аудиосегменте модель обучается предсказывать токен <|nospeech|>, указывающий на это. Следующий токен определяет задачу (либо транскрипция, либо перевод) с помощью токена <|transcribe|> или <|translate|>. После этого мы указываем, нужно ли предсказывать временные метки, добавляя токен <|notimestamps|> в случае их отсутствия. На этом этапе задача и желаемый формат полностью определены, и начинается вывод. Для предсказания временных меток мы предсказываем время относительно текущего аудиосегмента, квантуя все времена с точностью до 20 миллисекунд, что соответствует естественному временному разрешению моделей Whisper, и добавляем дополнительные токены в наш словарь для каждого из этих значений. Мы чередуем их предсказание с токенами субтитров: токен времени начала предсказывается перед текстом каждого субтитра, а токен времени окончания — после. Когда последний сегмент транскрипта только частично включен в текущий 30-секундный аудиофрагмент, мы предсказываем только токен времени начала для этого сегмента в режиме временных меток, чтобы указать, что последующее декодирование должно выполняться на аудиоокне, выровненном по этому времени; в противном случае мы обрезаем аудио, чтобы не включать этот сегмент. Наконец, мы добавляем токен <|endoftranscript|>. Мы маскируем потерю обучения только на предыдущем текстовом контексте и обучаем модель предсказывать все остальные токены. Обзор нашего формата и настройки обучения представлен на рисунке 1.

#### 2.4. Детали обучения

Мы обучаем набор моделей различных размеров, чтобы изучить свойства масштабирования Whisper. Обзор представлен в таблице 1. Мы обучаем с параллелизмом данных на ускорителях, используя FP16 с динамическим масштабированием потерь и сохранением контрольных точек активации (Griewank & Walther, 2000; Chen et al., 2016). Модели обучались с использованием AdamW (Loshchilov & Hutter, 2017) и обрезкой нормы градиента (Pascanu et al., 2013) с линейным затуханием скорости обучения до нуля после разогрева в течение первых 2048 обновлений. Использовался размер батча в 256 сегментов, и модели обучались в течение 220 обновлений, что составляет от двух до трех проходов по набору данных. Поскольку обучение проводится всего в течение нескольких эпох, переобучение не является серьезной проблемой, и мы не используем аугментацию данных или регуляризацию, полагаясь вместо этого на разнообразие, содержащееся в таком большом наборе данных, чтобы способствовать обобщению и устойчивости. Полные гиперпараметры обучения приведены в Приложении F.