## [Robust Speech Recognition via Large-Scale Weak Supervision](https://arxiv.org/abs/2212.04356)

### Аннотация
Мы изучаем возможности систем обработки речи, обученных просто предсказывать большие объемы транскриптов аудиозаписей в Интернете. При масштабировании до 680 000 часов многоязычного и многозадачного наблюдения полученные модели хорошо обобщаются на стандартные эталоны и часто конкурируют с предыдущими результатами, полученными при полном наблюдении, но в условиях нулевого переноса без необходимости какой-либо настройки. При сравнении с человеком модели приближаются к нему по точности и надежности. Мы выпускаем модели и код вывода, чтобы они послужили основой для дальнейшей работы над надежной обработкой речи.

### 1.Введение
Прогресс в области распознавания речи был стимулирован разработкой методов предварительного обучения без контроля, примером которых может служить Wav2Vec 2.0 (Baevski et al., 2020). Поскольку эти методы обучаются непосредственно на необработанном аудио, не требуя человеческих меток, они могут продуктивно использовать большие наборы данных немаркированной речи и были быстро увеличены до 1 000 000 часов обучающих данных (Zhang et al., 2021), что намного больше, чем 1 000 или около того часов, типичных для академических контролируемых наборов данных. При точной настройке на стандартных эталонах этот подход позволил улучшить положение дел в данной области, особенно в условиях низкого объема данных. 

Эти предварительно обученные аудиокодеры обучаются высококачественным представлениям речи, но поскольку они являются чисто неконтролируемыми, им не хватает эквивалентно эффективного декодера, отображающего эти представления в пригодные для использования выходные данные, что требует этапа доводки для реального выполнения такой задачи, как распознавание речи1 . Это, к сожалению, ограничивает их полезность и влияние, поскольку тонкая настройка может быть сложным процессом, требующим квалифицированного специалиста. Необходимость тонкой настройки сопряжена с дополнительным риском. Методы машинного обучения очень хорошо умеют находить закономерности в обучающем наборе данных, которые повышают производительность при обработке данных из того же набора. Однако некоторые из этих закономерностей являются хрупкими и надуманными и не распространяются на другие наборы данных и распределения. Особенно тревожный пример - Рэдфорд и др. (2021) зафиксировали увеличение точности классификации объектов на 9,2 % при тонкой настройке модели компьютерного зрения на наборе данных ImageNet (Russakovsky et al., 2015), не заметив при этом никакого улучшения средней точности при классификации тех же объектов на семи других наборах данных естественных изображений. Модель, которая достигает «сверхчеловеческой» производительности при обучении на одном наборе данных, может совершать множество базовых ошибок при оценке на другом, возможно, именно потому, что она использует те специфические особенности набора данных, на которые человек не обращает внимания (Geirhos et al., 2020). 

Это говорит о том, что, хотя предварительное обучение без контроля значительно улучшило качество аудиокодеров, отсутствие эквивалентно качественного предварительно обученного декодера в сочетании с рекомендованным протоколом тонкой настройки для конкретного набора данных является решающим недостатком, который ограничивает их полезность и устойчивость. Цель системы распознавания речи должна состоять в том, чтобы надежно работать «из коробки» в широком диапазоне сред, не требуя контролируемой тонкой настройки декодера для каждого распределения развертывания. 

Как показали работы Narayanan et al. (2018), Likhomanenko et al. (2020) и Chan et al. (2021), системы распознавания речи, предварительно обученные под надзором на многих наборах данных/доменах, демонстрируют более высокую устойчивость и гораздо эффективнее обобщают данные, чем модели, обученные на одном источнике. В этих работах это достигается путем объединения как можно большего количества существующих высококачественных наборов данных для распознавания речи. Однако до сих пор в свободном доступе находится лишь небольшое количество таких данных. SpeechStew (Chan et al., 2021) объединяет 7 уже существующих наборов данных общим объемом 5 140 часов наблюдения. Хотя это и не так уж мало, но все же ничтожно мало по сравнению с ранее упомянутыми 1 000 000 часов немаркированных речевых данных, использованных в работе Zhang et al. (2021).  

Признавая ограниченный размер существующих высококачественных контролируемых наборов данных, в последнее время предпринимаются попытки создать более крупные наборы данных для распознавания речи. Ослабив требование золотого стандарта человеческой валидации транскриптов, Чен и др. (2021) и Гальвес и др. (2021) используют сложные автоматизированные конвейеры для масштабирования слабо контролируемого распознавания речи до 10 000 и 30 000 часов более шумных обучающих данных. Такой компромисс между качеством и количеством часто оказывается правильным решением. Хотя распознавание речи до сих пор изучалось недостаточно, недавние работы в области компьютерного зрения показали, что переход от золотых стандартов краудсорсинговых наборов данных, таких как ImageNet (Russakovsky et al., 2015), к гораздо большим, но слабо контролируемым наборам данных значительно улучшает надежность и обобщение моделей (Mahajan et al., 2018; Kolesnikov et al., 2020). 

Однако эти новые наборы данных лишь в несколько раз больше, чем сумма существующих высококачественных наборов данных, и все еще намного меньше, чем предыдущие работы без надзора. В данной работе мы восполняем этот пробел, масштабируя слабо контролируемое распознавание речи на следующий порядок величины до 680 000 часов маркированных аудиоданных. Мы назвали наш подход Whisper2 . Мы демонстрируем, что модели, обученные в таком масштабе, хорошо переносятся на существующие наборы данных zeroshot, устраняя необходимость в какой-либо тонкой настройке, специфичной для данного набора данных, для достижения высококачественных результатов. 

== +5203

Помимо масштаба, наша работа также направлена на расширение сферы применения предварительного обучения со слабым контролем за распознаванием речи, которое не ограничивается только английским языком, но может быть многоязычным и многозадачным. Из этих 680 000 часов аудио 117 000 часов охватывают 96 других языков. Набор данных также включает 125 000 часов данных перевода X→en. Мы обнаружили, что для достаточно больших моделей совместное многоязычное и многозадачное обучение не имеет недостатков и даже приносит пользу.  

Наша работа показывает, что простое масштабирование предварительного обучения под слабым контролем до сих пор недооценивалось для распознавания речи. Мы достигли этих результатов без использования методов самоконтроля и самообучения, которые были основой недавних крупномасштабных работ по распознаванию речи. Чтобы послужить основой для дальнейших исследований в области надежного распознавания речи, мы публикуем код и модели вывода на следующем URL: https://github.com/openai/ whisper.  

### 2. Подход 
#### 2.1. Обработка данных 
Следуя тенденции последних работ по использованию текстов из интернета для обучения систем машинного обучения, мы используем минималистский подход к предварительной обработке данных. В отличие от многих работ по распознаванию речи, мы обучаем модели Whisper предсказывать сырой текст транскриптов без какой-либо существенной стандартизации, полагаясь на выразительность моделей «последовательность-последовательность», чтобы научиться сопоставлять произнесенные слова с их транскрибированной формой. Это упрощает конвейер распознавания речи, поскольку устраняет необходимость в отдельном шаге обратной нормализации текста для получения натуралистичных транскрипций. 

Мы создаем набор данных из аудиозаписей, которые сопоставляются с транскриптами в Интернете. В результате мы получаем очень разнообразную базу данных, охватывающую широкий спектр аудиозаписей из различных сред, систем записи, дикторов и языков. В то время как разнообразие в качестве аудио может помочь обучить модель быть надежной, разнообразие в качестве транскрипта не столь полезно. Первоначальная проверка показала наличие большого количества некачественных транскриптов в исходном наборе данных. Чтобы решить эту проблему, мы разработали несколько методов автоматической фильтрации для улучшения качества транскриптов. 

Многие транскрипты в Интернете на самом деле не являются человеческими, а представляют собой результат работы существующих ASR-систем. Недавние исследования показали, что обучение на наборах данных, состоящих из смешанных человеческих и машинных данных, может значительно ухудшить производительность систем перевода (Ghorbani et al., 2021). Чтобы избежать изучения «транскриптового языка», мы разработали множество эвристик для обнаружения и удаления машинных транскриптов из обучающего набора данных. Многие существующие системы ASR выводят только ограниченное подмножество письменного языка, удаляя или нормализуя аспекты, которые трудно предсказать на основе только звуковых сигналов, такие как сложная пунктуация (восклицательные знаки, запятые и знаки вопроса), форматирование пробелов, таких как абзацы, или стилистические аспекты, такие как капитализация. Очень маловероятно, что транскрипт, состоящий из всех прописных или всех строчных букв, был создан человеком. Хотя многие системы ASR включают в себя некоторый уровень обратной нормализации текста, он часто является простым или основанным на правилах и все еще обнаруживается по другим необработанным аспектам, таким как отсутствие запятых. 

Мы также используем детектор языка аудио, который был создан путем тонкой настройки прототипной модели, обученной на прототипной версии набора данных на VoxLingua107 (Valk & Alumae¨ , 2021), чтобы убедиться, что разговорный язык соответствует языку транскрипта в соответствии с CLD2. Если они не совпадают, мы не включаем пару (аудио, транскрипт) в качестве обучающего примера для распознавания речи в набор данных. Мы делаем исключение, если язык транскрипта - английский, и добавляем эти пары в набор данных как примеры для обучения переводу речи X→en. Мы используем нечеткую дедупликацию текстов транскриптов, чтобы уменьшить количество дублирования и автоматически сгенерированного контента в наборе обучающих данных. 

Мы разбиваем аудиофайлы на 30-секундные сегменты, сопоставляя их с подмножеством транскрипта, которое встречается в этом временном сегменте. Мы обучаемся на всех аудиофайлах, включая сегменты, в которых отсутствует речь (хотя и с вероятностью субсэмплирования), и используем эти сегменты в качестве обучающих данных для обнаружения голосовой активности.

Для дополнительной фильтрации после обучения начальной модели мы собрали информацию об уровне ошибок на обучающих источниках данных и провели ручную проверку этих источников данных, сортируя их по сочетанию высокого уровня ошибок и размера источника данных, чтобы эффективно выявить и удалить некачественные. Эта проверка показала наличие большого количества только частично расшифрованных или плохо выровненных/выровненных транскриптов, а также оставшихся некачественных титров, сгенерированных машиной, которые эвристика фильтрации не обнаружила. 

Чтобы избежать контаминации, мы провели дедупликацию на уровне транскриптов между обучающим набором данных и оценочными наборами, которые, по нашему мнению, имели более высокий риск совпадения, а именно TED-LIUM 3 (Hernandez et al., 2018). 

== +4935

#### 2.2. Модель 

Поскольку основное внимание в нашей работе уделяется изучению возможностей крупномасштабного контролируемого предварительного обучения для распознавания речи, мы используем готовую архитектуру, чтобы избежать путаницы в наших выводах, связанных с улучшением модели. Мы выбрали кодер-декодер Transformer (Vaswani et al., 2017), поскольку эта архитектура хорошо зарекомендовала себя при надежном масштабировании. Все аудиозаписи передискретизированы до 16 000 Гц, а 80-канальное представление спектрограммы в логмагнетическом формате Mel вычисляется в 25-миллисекундных окнах с шагом 10 миллисекунд. Для нормализации характеристик мы глобально масштабируем входные данные в диапазоне от -1 до 1 с приблизительно нулевым средним значением по набору данных для предварительного обучения. Кодер обрабатывает это входное представление с помощью небольшого стебля, состоящего из двух слоев свертки с шириной фильтра 3 и функцией активации GELU (Hendrycks & Gimpel, 2016), где второй слой свертки имеет шаг 2. Синусоидальные позиционные вкрапления добавляются к выходу стебля, после чего применяются блоки трансформации кодера. В трансформаторе используются остаточные блоки предварительной активации (Child et al., 2019), а к выходу кодера применяется нормализация последнего слоя. Декодер использует выученные позиционные вкрапления и связанные представления входных и выходных лексем (Press & Wolf, 2017). Кодер и декодер имеют одинаковую ширину и количество блоков трансформации. На рисунке 1 представлена архитектура модели. 

Мы используем тот же байтовый токенизатор текста BPE, что и в GPT2 (Sennrich et al., 2015; Radford et al., 2019) для англоязычных моделей и изменяем словарь (но сохраняем тот же размер) для многоязычных моделей, чтобы избежать чрезмерной фрагментации на других языках, поскольку словарь BPE в GPT-2 только английский. 

#### 2.3. Многозадачный формат 

Хотя предсказание того, какие слова были произнесены в данном аудиофрагменте, является основной частью задачи полного распознавания речи и широко изучается в исследованиях, это не единственная часть. Полнофункциональная система распознавания речи может включать в себя множество дополнительных компонентов, таких как определение активности голоса, диктофонная запись и обратная нормализация текста. Эти компоненты часто рассматриваются отдельно, что приводит к созданию относительно сложной системы вокруг основной модели распознавания речи. Чтобы уменьшить эту сложность, мы хотели бы, чтобы одна модель выполняла весь цикл обработки речи, а не только основную часть распознавания. Важным моментом здесь является интерфейс для модели. Существует множество различных задач, которые могут быть выполнены на одном и том же входном аудиосигнале: транскрипция, перевод, определение активности голоса, выравнивание и идентификация языка - вот лишь некоторые примеры. 

Чтобы такое сопоставление «один ко многим» работало с одной моделью, необходима определенная форма спецификации задач. Мы используем простой формат для задания всех задач и информации об условиях в виде последовательности входных лексем для декодера. Поскольку наш декодер - это языковая модель с аудиоусловиями, мы также обучаем его учитывать историю текста транскрипта в надежде, что он научится использовать более дальний текстовый контекст для разрешения неоднозначного аудио. В частности, с некоторой вероятностью мы добавляем в контекст декодера текст транскрипта, предшествующий текущему аудиосегменту. Мы обозначаем начало предсказания маркером <|startoftranscript|>. Сначала мы предсказываем язык, на котором говорят, который представлен уникальным маркером для каждого языка в нашем обучающем наборе (всего 99). Эти языковые цели взяты из вышеупомянутой модели VoxLingua107. В случае отсутствия речи в аудиофрагменте модель обучается предсказывать лексему <|nospeech|>, указывающую на это. Следующая лексема указывает на задачу (транскрипция или перевод) с помощью лексем <|transcribe|> или <|translate|>. После этого мы указываем, нужно ли предсказывать временные метки или нет, включая токен <|notimestamps|> для этого случая. На этом этапе задача и желаемый формат полностью определены, и начинается вывод. Для предсказания временных меток мы предсказываем время относительно текущего аудиофрагмента, квантуя все времена с точностью до 20 миллисекунд, что соответствует собственному временному разрешению моделей Whisper, и добавляем дополнительные лексемы в наш словарь для каждого из них. Мы чередуем их предсказание с предсказанием лексем титров: лексема времени начала предсказывается перед текстом каждого титра, а лексема времени окончания - после. Если заключительный сегмент транскрипта лишь частично включен в текущий 30-секундный аудиофрагмент, мы предсказываем для него только маркер времени начала в режиме временной метки, чтобы указать, что последующее декодирование должно выполняться в аудиоокне, выровненном по этому времени, в противном случае мы усекаем аудио, чтобы не включать сегмент. Наконец, мы добавляем маркер <|endoftranscript|>. Мы маскируем потери при обучении только для предыдущего контекстного текста и обучаем модель предсказывать все остальные лексемы. На рисунке 1 представлен обзор нашего формата и обучающей установки.

== +4955


#### 2.4. Детали обучения 

Мы обучаем набор моделей разного размера, чтобы изучить свойства масштабирования Whisper. Обзор приведен в таблице 1. Мы обучаем с параллелизмом данных на разных ускорителях, используя FP16 с динамическим масштабированием потерь и активацией контрольных точек (Griewank & Walther, 2000; Chen et al., 2016). Модели обучались с помощью AdamW (Loshchilov & Hutter, 2017) и градиентного обрезания нормы (Pascanu et al., 2013) с линейным спадом скорости обучения до нуля после разминки в течение первых 2048 обновлений. Размер партии составлял 256 сегментов, и модели обучались в течение 220 обновлений, что составляет от двух до трех проходов по набору данных. Поскольку обучение длится всего несколько эпох, чрезмерная подгонка не вызывает больших опасений, и мы не используем никаких дополнений или регуляризации данных, а полагаемся на разнообразие, содержащееся в таком большом наборе данных, чтобы стимулировать обобщение и устойчивость. Полные гиперпараметры обучения приведены в приложении F.

== +965